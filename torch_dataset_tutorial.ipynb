{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-pedestrian Dataset Tutorial\n",
    "\n",
    "Author: [Zhe Huang](https://github.com/tedhuang96) <br/>\n",
    "Date: Nov. 8, 2020 <br/>\n",
    "The code base is heavily borrowed from [Social-STGCNN](https://github.com/abduallahmohamed/Social-STGCNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up source files, packages and arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m_wnepU1gHFm",
    "outputId": "8539068f-c234-4e00-8250-e013391b5d98",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clone source files into Colab.\n",
    "!git clone https://github.com/tedhuang96/torch_data_tutorial.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and check GPU.\n",
    "# \n",
    "### expected output: ###\n",
    "### cuda:0 ### or ### cpu ###\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "from torch_data_tutorial.utils import anorm, create_datasets\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write arguments.\n",
    "# \n",
    "### expected output: ###\n",
    "### Namespace(attn_mech='auth', dataset='zara1', obs_seq_len=8, pred_seq_len=12) ###\n",
    "\n",
    "def arg_parse_notebook(obs_seq_len, pred_seq_len, dataset, attn_mech):\n",
    "    obs_seq_len, pred_seq_len = str(obs_seq_len), str(pred_seq_len)\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--obs_seq_len', type=int, default=0)\n",
    "    parser.add_argument('--pred_seq_len', type=int, default=0)\n",
    "    parser.add_argument('--dataset', default='eth',\n",
    "                        help='eth,hotel,univ,zara1,zara2')\n",
    "    parser.add_argument('--attn_mech', default='glob_kip',\n",
    "                        help='attention mechanism: glob_kip, auth')\n",
    "    args_list = ['--obs_seq_len', obs_seq_len, '--pred_seq_len', pred_seq_len, '--dataset', dataset, '--attn_mech', attn_mech]\n",
    "    return parser.parse_args(args_list)\n",
    "\n",
    "obs_seq_len, pred_seq_len, dataset, attn_mech = 8, 12, 'zara1', 'auth'\n",
    "args = arg_parse_notebook(obs_seq_len, pred_seq_len, dataset, attn_mech)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduce multi-pedestrian datasets.\n",
    "\n",
    "Below are snapshots from ETH/UCY datasets. These datasets provide a [benchmark](http://trajnet.stanford.edu/) for multi-pedestrian trajectory prediction algorithms which are focused on pedestrian interaction mechanism. ETH contains two scenarios and UCY contains other three. They recorded and annotated pedestrian trajectories from a bird's eye view. The annotations include frame_id, pedestrian_id, x, y. \n",
    "![title](multi_pedestrian_scenarios.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load an example dataset into pandas.\n",
    "# \n",
    "### expected output: ###\n",
    "### filename:  torch_data_tutorial/datasets/zara1/train/train.txt ###\n",
    "### data table: <traj_data_table> ###\n",
    "pkg_path = 'torch_data_tutorial'\n",
    "traj_data_folder = join(pkg_path, 'datasets', args.dataset, 'train') # may change 'datasets' to 'datasets_loo' for the leave-one-out setting.\n",
    "traj_data_filepath = join(traj_data_folder, listdir(traj_data_folder)[0])\n",
    "traj_data_table = pd.read_csv(traj_data_filepath, sep=\"\\t\", header=None)\n",
    "traj_data_table.columns = [\"frame_id\", \"ped_id\", \"x\", \"y\"]\n",
    "print('filename: ', traj_data_filepath)\n",
    "print('data table: ')\n",
    "print(traj_data_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot a pedestrian's trajectory.\n",
    "# \n",
    "### expected output: ###\n",
    "### <ped_traj> ###\n",
    "### A trajectory plot corresponding to the pedestrian we queried. ###\n",
    "\n",
    "ped_traj = traj_data_table.loc[traj_data_table['ped_id'] == 1]\n",
    "print(ped_traj)\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(ped_traj['x'], ped_traj['y'], 'o-')\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot pedestrian positions in a frame.\n",
    "# \n",
    "### expected output: ###\n",
    "### <frame_pos> ###\n",
    "### A scatter plot with each point representing a pedestrian position in that specific frame. ###\n",
    "\n",
    "frame_pos = traj_data_table.loc[traj_data_table['frame_id']==0]\n",
    "print(frame_pos)\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(frame_pos['x'], frame_pos['y'], 'o')\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation of pedestrian motions in a 20-frame period.\n",
    "# \n",
    "### expected output: ###\n",
    "### A scatter plot animation with each point representing a pedestrian position. ###\n",
    "### blue means the motion is during observation period, and ren means the motion is during prediction period. ###\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim((0, 16))\n",
    "ax.set_ylim((2, 9))\n",
    "ped_pos, = ax.plot([], [],'o')\n",
    "plt.close()\n",
    "\n",
    "def init():\n",
    "    ped_pos.set_data([], [])\n",
    "    return (ped_pos,)\n",
    "\n",
    "def animate(i):\n",
    "    frame_pos = traj_data_table.loc[traj_data_table['frame_id'] == i*10]\n",
    "    x, y = frame_pos['x'], frame_pos['y']\n",
    "    ped_pos.set_data(x, y)\n",
    "    if i > 8:\n",
    "        ped_pos.set_color('C3') # red\n",
    "    return (ped_pos,)\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=20, interval=200, blit=True)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We are interested in predicting pedestrian motion in the later 12 time steps\n",
    "# given the observation in the first 8 time steps.\n",
    "#\n",
    "# We filter out common pedestrians across all 20 time steps,\n",
    "# and their trajectories in these 20 time steps formed a data sample.\n",
    "# \n",
    "### expected output: ###\n",
    "### ped_ids shown up in all 20 frames: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 8.0] ###\n",
    "### Two trajectory plots ###\n",
    "\n",
    "ped_ids_20_frames = []\n",
    "for i in range(0, 19):\n",
    "    ped_ids_20_frames.append(set(traj_data_table.loc[traj_data_table['frame_id'] == i*10]['ped_id']))\n",
    "common_ped_ids_20_frames = list(set.intersection(*ped_ids_20_frames))\n",
    "print('ped_ids shown up in all 20 frames:', common_ped_ids_20_frames)\n",
    "\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "for common_ped_id in common_ped_ids_20_frames[:8]: \n",
    "    ped_traj_pred = traj_data_table.loc[(traj_data_table['ped_id'] == common_ped_id) & (traj_data_table['frame_id'] <= 190)]\n",
    "    ax.plot(ped_traj_pred['x'], ped_traj_pred['y'], 'o-')\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "for common_ped_id in common_ped_ids_20_frames[:8]: \n",
    "    ped_traj_pred = traj_data_table.loc[(traj_data_table['ped_id'] == common_ped_id) & (traj_data_table['frame_id'] >= 70) \\\n",
    "                                       & (traj_data_table['frame_id'] <= 190)]\n",
    "    ax.plot(ped_traj_pred['x'], ped_traj_pred['y'], 'o-', c='C3') # red\n",
    "    ped_traj_obs = traj_data_table.loc[(traj_data_table['ped_id'] == common_ped_id) & (traj_data_table['frame_id'] < 80)]\n",
    "    ax.plot(ped_traj_obs['x'], ped_traj_obs['y'], 'o-',c='C0') # blue\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Customize a torch dataset for multi-pedestrian trajectory data.\n",
    "\n",
    "Deep learning has become increasingly effective in learning patterns from data in recent years. We will use [PyTorch](https://pytorch.org) as our deep learning framework to handle multi-pedestrian trajectory prediction tasks.\n",
    "\n",
    "The first step to accomplish the task is to build a dataset that algorithms developed in Pytorch can recognize as input. Check out TrajectoryDataset class in utils.py, which we will be using to create the dataset.\n",
    "\n",
    "Extension: [Mini-batch Gradient Descent](https://xzz201920.medium.com/gradient-descent-stochastic-vs-mini-batch-vs-batch-vs-adagrad-vs-rmsprop-vs-adam-3aa652318b0d).\n",
    "\n",
    "![title](sgd_bgd_mbgd.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### expected output: ###\n",
    "# ['torch_data_tutorial/datasets/zara1/train/train.txt']\n",
    "# Processing Data .....\n",
    "# 100%\n",
    "# 503/503 [00:11<00:00, 42.84it/s]\n",
    "#\n",
    "# ['torch_data_tutorial/datasets/zara1/val/val.txt']\n",
    "# Processing Data .....\n",
    "# 100%\n",
    "# 85/85 [00:05<00:00, 15.75it/s]\n",
    "#\n",
    "# ['torch_data_tutorial/datasets/zara1/test/crowds_zara01.txt']\n",
    "# Processing Data .....\n",
    "# 100%\n",
    "# 602/602 [00:13<00:00, 43.06it/s]\n",
    "\n",
    "pkg_path = 'torch_data_tutorial'\n",
    "dsets = create_datasets(args, pkg_path, save_datasets=True)\n",
    "dloader_train = DataLoader(\n",
    "        dsets['train'],\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for visualization.\n",
    "\n",
    "def get_batch_sample(loader_test, device='cuda:0'):\n",
    "    batch_count = 1\n",
    "    for cnt, batch in enumerate(loader_test):\n",
    "        batch_count += 1\n",
    "        # Get data\n",
    "        batch = [tensor.to(device) for tensor in batch]\n",
    "        # ** Name of variables in a batch\n",
    "        # * obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, non_linear_ped,\\\n",
    "        # *    loss_mask, V_obs, A_obs, V_tr, A_tr = batch\n",
    "        var_names = [\n",
    "            'obs_traj', 'pred_traj_gt', 'obs_traj_rel', 'pred_traj_gt_rel', 'non_linear_ped',\n",
    "            'loss_mask', 'V_obs', 'A_obs', 'V_tr', 'A_tr']\n",
    "        for var_name_i, batch_i in zip(var_names, batch):\n",
    "            if var_name_i == 'obs_traj' and batch_i.shape[1] == 5: # get five-pedestrian case\n",
    "                return batch\n",
    "\n",
    "def visualize_dataloader(data_loader, attn_scale=1., device='cuda:0'):\n",
    "    \"\"\"\n",
    "    visualize data and attention in a batch generated by data_loader.\n",
    "    \"\"\"\n",
    "    batch = get_batch_sample(data_loader, device=device)\n",
    "    obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, non_linear_ped,\\\n",
    "        loss_mask, V_obs, A_obs, V_tr, A_tr = batch\n",
    "    obs_ts = 2 # time of attention\n",
    "    attn = A_obs[0, obs_ts]  # (num_peds, num_peds)\n",
    "    fig, ax = plt.subplots()\n",
    "    colors = 'rgggg'\n",
    "#     attn_scale = 1. # size of attention circle\n",
    "    for ped_i in range(obs_traj.shape[1]):\n",
    "        x_obs_i = obs_traj[0, ped_i].to('cpu')\n",
    "        x_gt_i = pred_traj_gt[0, ped_i].to('cpu')\n",
    "        x_concat_i = torch.cat([x_obs_i, x_gt_i], dim=1)\n",
    "        ax.plot(x_concat_i[0, :], x_concat_i[1, :], 'o-',c='C3')\n",
    "        ax.plot(x_obs_i[0, :], x_obs_i[1, :], 'o-', c='C0')\n",
    "        ax.plot(x_obs_i[0, obs_ts], x_obs_i[1, obs_ts], 'ko-')\n",
    "        attn_i = plt.Circle(\n",
    "            x_obs_i[:, obs_ts], abs(attn[ped_i, 0])*attn_scale, color=colors[ped_i], fill=False)\n",
    "        ax.add_artist(attn_i)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    plt.show()\n",
    "    \n",
    "def dataset_format():\n",
    "    \"\"\"\n",
    "    Documentation on structure of a batch from the dataset.\n",
    "    - batch # list.\n",
    "\n",
    "        - obs_traj # global positions in observation. # tensor: (1, num_peds, 2, 8)\n",
    "        # 2 means x and y. # 8 means obs_period.\n",
    "\n",
    "        - pred_traj_gt # ground truth global positions in prediction. # tensor: (1, num_peds, 2, 12)\n",
    "        # 12 means pred_period.\n",
    "\n",
    "        - obs_traj_rel # displacement in observation. # tensor: (1, num_peds, 2, 8)\n",
    "        # obs_traj_rel[:,:,:,0] is zero. # obs_traj_rel[:,:,:,1:]=obs_traj[:,:,:,1:]-obs_traj[:,:,:,:-1]\n",
    "\n",
    "        - pred_traj_gt_rel # ground truth displacement in prediction. # tensor: (1, num_peds, 2, 12)\n",
    "        # pred_traj_gt_rel[:,:,:,0]=pred_traj_gt[:,:,:,0]-obs_traj[:,:,:,-1]\n",
    "\n",
    "        - non_linear_ped # 0-1 vector indicates whether motion is nonlinear. # tensor: (1, num_peds)\n",
    "\n",
    "        - loss_mask # all-one tensor. # tensor: (1, num_peds, 20) # 20 means full_period.\n",
    "\n",
    "        - V_obs # vertices represent displacement of pedestrians in observation.\n",
    "        # tensor: (1, 8, num_peds, 2) # V_obs.permute(0,2,3,1) == obs_traj_rel\n",
    "\n",
    "        - A_obs # Adjacency matrix of pedestrians in observation. # tensor: (1, 8, num_peds, num_peds)\n",
    "\n",
    "        - V_tr # ground truth vertices represent displacement of pedestrians in prediction.\n",
    "        # tensor: (1, 12, num_peds, 2)\n",
    "\n",
    "        - A_tr # ground truth Adjacency matrix of pedestrians in prediction.\n",
    "        # tensor: (1, 12, num_peds, num_peds)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### expected output: ###\n",
    "### Visualization on trajectories and attention ###\n",
    "\n",
    "visualize_dataloader(dloader_train, attn_scale=1.5, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and evaluate a [Social-STGCNN](https://arxiv.org/pdf/2002.11927.pdf) model on a multi-pedestrian trajectory prediction task.\n",
    "\n",
    "![title](social-stgcnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the training file to train a Social-STGCNN model.\n",
    "\n",
    "!python torch_data_tutorial/train.py --num_epochs 100 --lr 0.01 --dataset zara1 --attn_mech auth --n_stgcnn 1 --n_txpcnn 5 --use_lrschd && echo \"Training on zara1 is complete.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test on the model you just trained.\n",
    "\n",
    "!python torch_data_tutorial/test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test on the model I trained using 200 epochs in this notebook.\n",
    "\n",
    "!python torch_data_tutorial/test.py --pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test on the model provided by the original github repo.\n",
    "\n",
    "!python torch_data_tutorial/test.py --original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction from the model and compare against ground truth.\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "pkg_path = 'torch_data_tutorial'\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.distributions.multivariate_normal as torchdist\n",
    "from torch_data_tutorial.utils import load_dataset\n",
    "from torch_data_tutorial.metrics import ade, fde, seq_to_nodes, nodes_rel_to_nodes_abs\n",
    "from torch_data_tutorial.model import social_stgcnn\n",
    "import copy\n",
    "\n",
    "### Select your model starts ###\n",
    "paths = ['torch_data_tutorial/checkpoint/auth-zara1-pretrained']\n",
    "# paths = ['torch_data_tutorial/checkpoint/auth-zara1']\n",
    "# paths = ['torch_data_tutorial/checkpoint/social-stgcnn-zara1']\n",
    "### Select your model ends ###\n",
    "\n",
    "def test(KSTEPS=20):\n",
    "    global loader_test,model\n",
    "    model.eval()\n",
    "    ade_bigls = []\n",
    "    fde_bigls = []\n",
    "    raw_data_dict = {}\n",
    "    step =0 \n",
    "    for batch in loader_test: \n",
    "        step+=1\n",
    "        #Get data\n",
    "        batch = [tensor.cuda() for tensor in batch]\n",
    "        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, non_linear_ped,\\\n",
    "            loss_mask,V_obs,A_obs,V_tr,A_tr = batch\n",
    "        num_of_objs = obs_traj_rel.shape[1]\n",
    "        V_obs_tmp =V_obs.permute(0,3,1,2)\n",
    "        V_pred,_ = model(V_obs_tmp,A_obs.squeeze())\n",
    "        V_pred = V_pred.permute(0,2,3,1)\n",
    "        V_tr = V_tr.squeeze()\n",
    "        A_tr = A_tr.squeeze()\n",
    "        V_pred = V_pred.squeeze()\n",
    "        num_of_objs = obs_traj_rel.shape[1]\n",
    "        V_pred,V_tr =  V_pred[:,:num_of_objs,:],V_tr[:,:num_of_objs,:] \n",
    "        sx = torch.exp(V_pred[:,:,2]) #sx\n",
    "        sy = torch.exp(V_pred[:,:,3]) #sy\n",
    "        corr = torch.tanh(V_pred[:,:,4]) #corr\n",
    "        cov = torch.zeros(V_pred.shape[0],V_pred.shape[1],2,2).cuda()\n",
    "        cov[:,:,0,0]= sx*sx\n",
    "        cov[:,:,0,1]= corr*sx*sy\n",
    "        cov[:,:,1,0]= corr*sx*sy\n",
    "        cov[:,:,1,1]= sy*sy\n",
    "        mean = V_pred[:,:,0:2]\n",
    "        mvnormal = torchdist.MultivariateNormal(mean,cov)\n",
    "        ### Rel to abs\n",
    "        #Now sample 20 samples\n",
    "        ade_ls = {}\n",
    "        fde_ls = {}\n",
    "        V_x = seq_to_nodes(obs_traj.data.cpu().numpy().copy()) # obs\n",
    "        V_x_rel_to_abs = nodes_rel_to_nodes_abs(V_obs.data.cpu().numpy().squeeze().copy(),\n",
    "                                                 V_x[0,:,:].copy())\n",
    "        V_y = seq_to_nodes(pred_traj_gt.data.cpu().numpy().copy()) # target\n",
    "        V_y_rel_to_abs = nodes_rel_to_nodes_abs(V_tr.data.cpu().numpy().squeeze().copy(),\n",
    "                                                 V_x[-1,:,:].copy())\n",
    "        raw_data_dict[step] = {}\n",
    "        raw_data_dict[step]['obs'] = copy.deepcopy(V_x_rel_to_abs)\n",
    "        raw_data_dict[step]['trgt'] = copy.deepcopy(V_y_rel_to_abs)\n",
    "        raw_data_dict[step]['pred'] = []\n",
    "        for n in range(num_of_objs): # num_peds\n",
    "            ade_ls[n]=[]\n",
    "            fde_ls[n]=[]\n",
    "        for k in range(KSTEPS):\n",
    "            fig_no_pred, ax_no_pred = plt.subplots()\n",
    "            fig, ax = plt.subplots()\n",
    "            V_pred = mvnormal.sample()\n",
    "            V_pred_rel_to_abs = nodes_rel_to_nodes_abs(V_pred.data.cpu().numpy().squeeze().copy(),\n",
    "                                                     V_x[-1,:,:].copy())\n",
    "            raw_data_dict[step]['pred'].append(copy.deepcopy(V_pred_rel_to_abs))\n",
    "           # print(V_pred_rel_to_abs.shape) #(12, 3, 2) = seq, ped, location\n",
    "            for n in range(num_of_objs):\n",
    "                pred = [] \n",
    "                target = []\n",
    "                obsrvs = [] \n",
    "                number_of = []\n",
    "                pred.append(V_pred_rel_to_abs[:,n:n+1,:])\n",
    "                target.append(V_y_rel_to_abs[:,n:n+1,:])\n",
    "                obsrvs.append(V_x_rel_to_abs[:,n:n+1,:])\n",
    "                number_of.append(1)\n",
    "                ### visualization starts ###\n",
    "                vis_obs = V_x_rel_to_abs[:,n] # (8, 2)\n",
    "                vis_target = target[-1][:,0] # (12, 2)\n",
    "                vis_pred = pred[-1][:,0] # (12, 2)\n",
    "                vis_obs_target = np.concatenate([vis_obs, vis_target], axis=0)\n",
    "                vis_obs_pred = np.concatenate([vis_obs, vis_pred], axis=0)\n",
    "                ax.plot(vis_obs_target[:,0], vis_obs_target[:,1], 'o-',c='C3')\n",
    "                ax.plot(vis_obs_pred[:,0], vis_obs_pred[:,1], 'o-',c='k')\n",
    "                ax.plot(vis_obs[:,0], vis_obs[:,1], 'o-',c='C0')\n",
    "                ax_no_pred.plot(vis_obs_target[:,0], vis_obs_target[:,1], 'o-',c='C3')\n",
    "                ax_no_pred.plot(vis_obs[:,0], vis_obs[:,1], 'o-',c='C0')\n",
    "                ### visualization ends ###\n",
    "                ade_ls[n].append(ade(pred,target,number_of))\n",
    "                fde_ls[n].append(fde(pred,target,number_of))\n",
    "            ax.set_aspect('equal', adjustable='box')\n",
    "            ax_no_pred.set_aspect('equal', adjustable='box')\n",
    "            plt.show()\n",
    "            break\n",
    "            \n",
    "        for n in range(num_of_objs):\n",
    "            ade_bigls.append(min(ade_ls[n]))\n",
    "            fde_bigls.append(min(fde_ls[n]))\n",
    "        \n",
    "        break\n",
    "\n",
    "    ade_ = sum(ade_bigls)/len(ade_bigls)\n",
    "    fde_ = sum(fde_bigls)/len(fde_bigls)\n",
    "    return ade_,fde_,raw_data_dict\n",
    "\n",
    "KSTEPS=5\n",
    "print(\"*\"*50)\n",
    "print('Number of samples:',KSTEPS)\n",
    "print(\"*\"*50)\n",
    "\n",
    "for feta in range(len(paths)):\n",
    "    ade_ls = [] \n",
    "    fde_ls = [] \n",
    "    path = paths[feta]\n",
    "    exps = glob.glob(path)\n",
    "    print('Model being tested are:',exps)\n",
    "\n",
    "    for exp_path in exps:\n",
    "        print(\"*\"*50)\n",
    "        print(\"Evaluating model:\",exp_path)\n",
    "\n",
    "        model_path = exp_path+'/val_best.pth'\n",
    "        args_path = exp_path+'/args.pkl'\n",
    "        with open(args_path,'rb') as f: \n",
    "            args = pickle.load(f)\n",
    "\n",
    "        args.attn_mech = 'auth' # force attn_mech when original implementation is used.\n",
    "\n",
    "        stats= exp_path+'/constant_metrics.pkl'\n",
    "        with open(stats,'rb') as f: \n",
    "            cm = pickle.load(f)\n",
    "        print(\"Stats:\",cm)\n",
    "        #Data prep     \n",
    "        obs_seq_len = args.obs_seq_len\n",
    "        pred_seq_len = args.pred_seq_len\n",
    "        loader_test = load_dataset(args, pkg_path, subfolder='test', num_workers=1)\n",
    "        #Defining the model \n",
    "        model = social_stgcnn(n_stgcnn =args.n_stgcnn,n_txpcnn=args.n_txpcnn,\n",
    "        output_feat=args.output_size,seq_len=args.obs_seq_len,\n",
    "        kernel_size=args.kernel_size,pred_seq_len=args.pred_seq_len).cuda()\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(\"Testing ....\")\n",
    "        ad,fd,raw_data_dic_= test()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "torch_dataset_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
